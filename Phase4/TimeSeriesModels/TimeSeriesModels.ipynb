{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First - Data Set Up\n",
    "\n",
    "Data is Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to March 2021 - [source](https://www.kaggle.com/mczielinski/bitcoin-historical-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from pmdarima.utils import decomposed_plot\n",
    "from pmdarima.arima import decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need pmdarima:\n",
    "# http://alkaline-ml.com/pmdarima/\n",
    "# !pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read df\n",
    "df = pd.read_csv('data/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows...\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Woah! Nulls! Is that going to be a problem?\n",
    "df.isna().sum()\n",
    "# 1231878 / 3997697 = .3ish (about 1/3 of our data is null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before worrying about nulls, let's change the timestamp\n",
    "# By setting unit to seconds, we get the date from the unix time\n",
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and set the index to be a datetime index\n",
    "df.set_index('Timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's look at how that impacted the time/index\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize all of our columns, to get a sense of the data\n",
    "for col in df.columns:\n",
    "    plt.figure(figsize=(16,6))\n",
    "    df[col].plot()\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see the general trends in our data, and we can see that, if those nulls are having an impact, it's not apparent in the plot!\n",
    "\n",
    "Regarding the Open / High / Low / Close plots - do you see much of a difference?\n",
    "\n",
    "Also! This is why we change our data to use the datetime object as the index - makes EVERYTHING easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the frequency of our data \n",
    "\n",
    "Also called downsampling or upsampling, depending on whether you're going to a less frequent or more frequent point in time.\n",
    "\n",
    "[Here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling) is a reference for resampling based on time frequency. (you can find the actual codes you can use as arguments in the resample function [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling to a daily cadence\n",
    "df_daily = df.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if we still have any nulls\n",
    "df_daily.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking into where those nulls are...\n",
    "df_daily.loc[df_daily['Open'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize just one column to see what changed on a daily scale\n",
    "plt.figure(figsize=(16,6))\n",
    "df_daily['Open'].plot()\n",
    "plt.title('Average Open Price - Daily')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see much of a change, just fewer data points - it's probably safe to say that we didn't lose much data by downsampling to a daily mean.\n",
    "\n",
    "But what about downsampling to monthly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're downsampling to month end, denoted by 'M'\n",
    "# If we wanted month start, we could use 'MS'\n",
    "df_monthly = df.resample('M').mean()\n",
    "df_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, visualizing the Average Opening price\n",
    "plt.figure(figsize=(16,6))\n",
    "df_monthly['Open'].plot()\n",
    "plt.title('Average Open Price - Monthly')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what about quarterly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're downsampling to month end, denoted by 'M'\n",
    "# If we wanted month start, we could use 'MS'\n",
    "df_quarterly = df.resample('Q').mean()\n",
    "df_quarterly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, visualizing the Average Opening price\n",
    "plt.figure(figsize=(16,6))\n",
    "df_quarterly['Open'].plot()\n",
    "plt.title('Average Open Price - Quarterly')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as would make sense, as you change the frequency of your data it changes the granularity (level of detail) that's conveyed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move forward, let's grab a single year from this data, 2017, at the daily frequency to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df[df.index.year == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017_daily = df_2017.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, visualizing the Average Opening price\n",
    "plt.figure(figsize=(16,6))\n",
    "df_2017_daily['Open'].plot()\n",
    "plt.title('Average Open Price - 2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "Introduction to stationarity from [_Forecasting: Principles and Practice_](https://otexts.com/fpp2/stationarity.html):\n",
    "\n",
    "> \"A stationary time series is one whose properties do not depend on the time at which the series is observed.14 Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time.\n",
    ">\n",
    "> \"Some cases can be confusing — a time series with cyclic behaviour (but with no trend or seasonality) is stationary. This is because the cycles are not of a fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be.\n",
    ">\n",
    "> \"In general, a stationary time series will have no predictable patterns in the long-term. Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible), with constant variance.\"\n",
    "\n",
    "And here's a [useful blog post](https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322) on the subject, where I found the below demonstrative image:\n",
    "\n",
    "![Examples of stationary and non-stationary processes, from the above medium blog](https://miro.medium.com/max/1400/1*tkx0_wwQ2JT7pSlTeg4yzg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's  get the rolling mean and rolling standard deviation, for the \n",
    "# opening price, using a 5-day window\n",
    "\n",
    "roll_mean = df_2017_daily['Open'].rolling(window=5, center=False).mean()\n",
    "roll_std = df_2017_daily['Open'].rolling(window=5, center=False).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "ax.plot(df_2017_daily['Open'], color='blue',label='Average Daily Opening Price')\n",
    "ax.plot(roll_mean, color='red', label='Rolling 5-Day Mean')\n",
    "ax.plot(roll_std, color='black', label='Rolling 5-Day Standard Deviation')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think this data is stationary? Why or why not?\n",
    "\n",
    " - \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a test for this!\n",
    "\n",
    "> **Augumented Dickey-Fuller test**: a hypothesis test, where we reject the null hypothesis (that a time series is non-stationary) if the test-statistic is less than the critical value\n",
    "\n",
    "[Documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html) for the Dickey-Fuller test in StatsModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's write out our null and alternative hypotheses (remember these??):\n",
    "\n",
    "Ho = \n",
    "\n",
    "Ha = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed in our Open column, since the test function expects a series:\n",
    "adfuller(df_2017_daily['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the output of this test:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've determined whether the data is stationary, let's decompose it\n",
    "\n",
    "# Using the decompose function from the arima model\n",
    "# Need to feed it an array, hence the .values attribute\n",
    "decomposed = decompose(df_2017_daily['Open'].values, 'additive', m=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the pieces using the arima model again\n",
    "decomposed_plot(decomposed, figure_kwargs={'figsize': (16, 10)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check the difference\n",
    "\n",
    "df_2017_daily_diff = df_2017_daily['Open'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now grabbing the rolling mean and std for the difference\n",
    "diff_roll_mean = df_2017_daily_diff.rolling(window=5, center=False).mean()\n",
    "diff_roll_std = df_2017_daily_diff.rolling(window=5, center=False).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "ax.plot(df_2017_daily_diff, color='blue',label='Difference')\n",
    "ax.plot(diff_roll_mean, color='red', label='Rolling Mean')\n",
    "ax.plot(diff_roll_std, color='black', label='Rolling Standard Deviation')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More or less stationary?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we logged the data?\n",
    "logged_2017_daily = np.log1p(df_2017_daily['Open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now grabbing the rolling mean and std for the difference\n",
    "log_roll_mean = logged_2017_daily.rolling(window=5, center=False).mean()\n",
    "log_roll_std = logged_2017_daily.rolling(window=5, center=False).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "ax.plot(logged_2017_daily, color='blue',label='Difference')\n",
    "ax.plot(log_roll_mean, color='red', label='Rolling Mean')\n",
    "ax.plot(log_roll_std, color='black', label='Rolling Standard Deviation')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More or less stationary?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "Any further ideas? \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now - Time to Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuring: ARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive (AR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An autoregressive (AR) model is when a value from a time series is regressed on previous values from the same time series.\n",
    "\n",
    "In words, the mathematical idea is the following:\n",
    "\n",
    "$$ \\text{Today = Constant + Slope} \\times \\text{Yesterday + Noise} $$\n",
    "\n",
    "Or, mathematically:\n",
    "$$\\large Y_t = \\mu + \\phi * Y_{t-1}+\\epsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average (MA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Moving Average (MA) model can be described as the weighted sum of today's and yesterday's noise.\n",
    "\n",
    "In words, the mathematical idea is the following:\n",
    "\n",
    "$$ \\text{Today = Mean + Noise + Slope} \\times \\text{Yesterday's Noise} $$\n",
    "\n",
    "Or, mathematically:\n",
    "$$\\large Y_t = \\mu +\\epsilon_t + \\theta * \\epsilon_{t-1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on these, based on the formulas:\n",
    "- If the slope is 0, the time series is a white noise model with mean $\\mu$\n",
    "- If the slope is not 0, the time series is autocorrelated and depends on the previous white noise process\n",
    "- Bigger slope means bigger autocorrelation\n",
    "- When there is a negative slope, the time series follow an oscillatory process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset who dis\n",
    "# Monthly Google search trends for 'taxes' in the US\n",
    "df_taxes = pd.read_csv('data/google-trends_taxes_us.csv')\n",
    "\n",
    "# Some quick clean up\n",
    "df_taxes.columns = ['counts']\n",
    "df_taxes = df_taxes.iloc[1:]\n",
    "df_taxes['counts'] = df_taxes['counts'].str.replace('<1', '0').astype(int)\n",
    "df_taxes.index = pd.to_datetime(df_taxes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMA Modeling \n",
    "\n",
    "Once you determine if your time series is stationary, you can model. There are 4 key steps: \n",
    "1. Model Identification - where you determine the properties of a time series then chose a structural form. Remember you're treating the data as a series of random variables. The basic types of ARIMA models are: \n",
    "    - AutoRegressive(AR)\n",
    "    - Moving Average(MA) \n",
    "    - Autoregressive Moving Average(ARMA)\n",
    "    - AutoRegressive Integrated Moving Average \n",
    "A time series may be primarily an autoregressive, moving average or combination of both. To identify which it is, you need to plot 2 key functions. \n",
    "    > Sample Autocorrelation Function(ACF) \n",
    "    > Sample Partial Autocorrelation Function(Partial ACF)\n",
    "    \n",
    "2. Parameter Estimation - Once you have identified the form of an ARIMA model, the next step is to estimate the coefficients or parameters of the model. You can use Regression and MLE to do this. \n",
    "\n",
    "3. Model Checking - The most widely used information criterion(checking the quality of your model) for Time series is AIC. You can compare different models with different numbers of lagged terms, white noise terms and how many times the time series was differenced and choose the model with the lowest AIC. \n",
    "\n",
    "4. Forecasting - Once the model is estimated you can forecast future values with the predict function. \n",
    "\n",
    "![](https://www.statisticshowto.com/wp-content/uploads/2016/11/lag-plot-linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial Autocorrelation\n",
    "The ACF shows the correlations between the elements of a time series as a function of their lags. The partial ACF shows the correlations between the elements of a time series for each lag, holding constant the impact of all other lags.\n",
    "\n",
    ">The basic idea of autocorrelation is simple: See how a series correlates with a \"lagged\" version of itself. If my sequence is $S_0 = (x_0, x_1, x_2, ... , x_n)$, then I can measure the Pearson correlation(multicollinearity) between the first $n-k + 1$ terms of $S_0$ and $S_{lag} = (x_k, x_{k+1}, x_{k+2}, ... , x_n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Function Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - \"The **autocorrelation function** is a function that represents autocorrelation of a time series as a function of the time lag.\"\n",
    "- The autocorrelation function tells interesting stories about trends and seasonality. For example, if the original time series repeats itself every five days, you would expect to see a spike in the autocorrelation function at 5 days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T17:23:35.743867Z",
     "start_time": "2020-10-16T17:23:35.386843Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.graphics.tsaplots as tsa\n",
    "\n",
    "tsa.plot_acf(df_taxes['counts'],lags=52);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T18:41:36.798858Z",
     "start_time": "2020-12-10T18:41:36.649776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Another view\n",
    "plt.figure(figsize=(20, 4))\n",
    "pd.plotting.autocorrelation_plot(df_taxes['counts']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal bands represent condfidence intervals, which are calculated by taking relevant z-scores of the standard normal distribution and dividing by the square root of the number of observations. What do these intervals mean? - anything outside confidence interval means not due to chance - reject null. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial-Autocorrelation Function Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> \"The **partial autocorrelation function** can be interpreted as a regression of the series against its past lags.\n",
    " \n",
    " > It helps you come up with a possible order for the auto regressive term. The terms can be interpreted the same way as a standard linear regression, that is the contribution of a change in that particular lag while holding others constant. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind partial Autocorrelation is to compare a series to a lagged version of itself while abstracting away from intermediate values. In effect, this amounts to exploring the correlations among residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T17:24:17.186767Z",
     "start_time": "2020-10-16T17:24:16.893580Z"
    }
   },
   "outputs": [],
   "source": [
    "tsa.plot_pacf(df_taxes['counts'],lags=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA MODELS:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ARIMA Time Series Model\n",
    "\n",
    "One of the most common methods used in time series forecasting is known as the ARIMA model, which stands for **AutoregRessive Integrated Moving Average**. ARIMA is a model that can be fitted to time series data in order to better understand or predict future points in the series.\n",
    "\n",
    "Let's have a quick introduction to ARIMA. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n",
    "\n",
    "### Number of AR (Auto-Regressive) terms (p): \n",
    "\n",
    "`p` is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to rain tomorrow if it has been raining for past 3 days. AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "\n",
    "### Number of Differences (d):\n",
    "\n",
    "`d` is the **Integrated** component of an ARIMA model. This value is concerned with the amount of differencing as it identifies the number of lag values to subtract from the current observation. Intuitively, this would be similar to stating that it is likely to rain tomorrow if the difference in amount of rain in the last *n* days is small. \n",
    "\n",
    "### Number of MA (Moving Average) terms (q): \n",
    "\n",
    "`q` is the moving average part of the model which is used to set the error of the model as a linear combination of the error values observed at previous time points in the past. MA terms form lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where `e(i)` is the difference between the moving average at ith instant and actual value.\n",
    "\n",
    "These three distinct integer values, (p, d, q), are used to parametrize ARIMA models. Because of that, ARIMA models are denoted with the notation `ARIMA(p, d, q)`. Together these three parameters account for seasonality, trend, and noise in datasets:\n",
    "\n",
    "* `(p, d, q)` are the non-seasonal parameters described above.\n",
    "* `(P, D, Q)` follow the same definition but are applied to the seasonal component of the time series. \n",
    "* The term `s` is the periodicity of the time series (4 for quarterly periods, 12 for yearly periods, etc.).\n",
    "\n",
    "A detailed article on these parameters is available [HERE](https://www.quantstart.com/articles/Autoregressive-Integrated-Moving-Average-ARIMA-p-d-q-Models-for-Time-Series-Analysis).\n",
    "\n",
    "The seasonal ARIMA method can appear daunting because of the multiple tuning parameters involved. In the next section, we will describe how to automate the process of identifying the optimal set of parameters for the seasonal ARIMA time series model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you generally will try to do for any time series analysis is:\n",
    "\n",
    "- Detrend your time series using differencing. ARMA models represent stationary processes, so we have to make sure there are no trends in our time series\n",
    "- Look at ACF and PACF of the time series\n",
    "- Decide on the AR, MA, and order of these models\n",
    "- Fit the model to get the correct parameters and use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# Note - we're back to regression metrics!\n",
    "def report_metrics(y_true, y_pred):\n",
    "    print(\"Explained Variance:\\n\\t\", metrics.explained_variance_score(y_true, y_pred))\n",
    "    print(\"MAE:\\n\\t\", metrics.mean_absolute_error(y_true, y_pred))\n",
    "    print(\"RMSE:\\n\\t\", metrics.mean_squared_error(y_true, y_pred, squared=False))\n",
    "    print(\"r^2:\\n\\t\", metrics.r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Back to taxes\n",
    "# Let's create a time period tracker\n",
    "df_taxes.insert(0, 't', range(len(df_taxes)))\n",
    "\n",
    "# Let's create a target area\n",
    "df_taxes['future'] = (df_taxes.index.year > 2017).astype('int')\n",
    "\n",
    "# Now plot\n",
    "plt.figure(figsize=(10,6))\n",
    "df_taxes.loc[df_taxes.future == 0, 'counts'].plot()\n",
    "df_taxes.loc[df_taxes.future == 1, 'counts'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_taxes.loc[df_taxes.future == 0, 't'].values.reshape(-1, 1)\n",
    "X_test = df_taxes.loc[df_taxes.future == 1, 't'].values.reshape(-1, 1)\n",
    "y_train = df_taxes.loc[df_taxes.future == 0, 'counts'].values\n",
    "y_test = df_taxes.loc[df_taxes.future == 1, 'counts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive approach\n",
    "pred_val = y_train[len(y_train)-1]\n",
    "y_pred = [pred_val] * len(y_test)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 0]['counts'])\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 1]['counts'])\n",
    "\n",
    "plt.plot(X_test, y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts? \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear trend approach\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_trend = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes.loc[df_taxes.future == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df_taxes['counts'])\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 0].index, y_trend)\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 1].index, y_pred, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMDArima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima\n",
    "pmdarima.auto_arima?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
